{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/MSML_602/blob/main/HW1/Homework2_PandasSQL_DATA602__121124586_Khetan.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwALoygGD617"
      },
      "source": [
        "# **HOMEWORK 2: PANDAS and SQL (TOTAL 80)**\n",
        "## **DUE: *SEPTEMBER 26, 2024 @ 11:59 PM***\n",
        "\n",
        "---------------------\n",
        "#### **DATASET DESCRIPTION**\n",
        "\n",
        "The [NCEI/WDS Global Significant Volcanic Eruptions Database](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ngdc.mgg.hazards:G10147) is a very comprehensive collection of +600 volcanic eruptions dating from 4360 BC to the present. Due to the nature of this assignment, we will be dealing with relatively newer volcanoes (in which some are still obviously still older than anyone on Earth currently). Each eruption in the database is classified as significant if it meets one or more criteria, such as causing fatalities, incurring **damage on property** (**+$1 million**), reaching a **Volcanic Explosivity Index (VEI)** of **6 or higher**, generating a tsunami, or being linked to a significant earthquake. The VEI is a scale that measures the explosiveness of volcanic eruptions, providing insight into the magnitude and potential consequences of the eruptions. The database includes detailed information on the location, type of volcano, last known eruption, VEI, casualties, property damage, and much more.\n",
        "![volcano](https://wikitravel.org/upload/shared//9/99/Volcano_de_Fuego_Banner.jpg)\n",
        "\n",
        "#### **Objective of the Assignment:**\n",
        "\n",
        "\n",
        "We will proceed directly to exploring these volcanic datasets, with the aim of enhancing our proficiency in Pandas and SQL.\n",
        "\n",
        "For your reference, the [Pandas Documentation](https://pandas.pydata.org/docs/user_guide/index.html) will be an invaluable resource. Additionally, we have provided several helpful links to assist you throughout this process. Please proceed with caution to avoid any potential pitf\n",
        "\n",
        "---------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sQEC_lpHYdq"
      },
      "source": [
        "### **DO NOT REMOVE ANY PART OF ANY OF THE QUESTIONS OR YOU LOSE CREDIT**\n",
        "### *No Hardcoding either*  \n",
        "### **REMEMBER TO SHOW ALL CODE OUTPUT (NO CREDIT OTHERWISE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrD3qZmJUp6C"
      },
      "source": [
        "### **Part 1: Maintenance  (25 POINTS TOTAL)**\n",
        "\n",
        "First, in this section, we will focus on the initial steps of the process.\n",
        "\n",
        "As is standard practice in Python programming, it is best to import necessary modules at the beginning of your script before writing any additional code. This approach ensures that all required libraries are available from the start."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "CruzvUOrKEoZ"
      },
      "outputs": [],
      "source": [
        "# Make sure these code blocks run properly and that you have properly installed the appropriate modules required.\n",
        "import pandas as pd\n",
        "import requests\n",
        "# import other libraries here\n",
        "\n",
        "# Don't remove this\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nWwbZ4jK5Wi"
      },
      "source": [
        "As you may have noticed, there is an additional library, apart from Pandas, called \"[requests](https://requests.readthedocs.io/en/latest/).\"\n",
        "\n",
        "**The requests library enables you to send HTTP requests to a server, retrieve content, and process it with ease.**  It is particularly user-friendly for beginners venturing into web scraping, which is crucial for collecting and constructing datasets. Additionally, we recommend exploring [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/), , a complementary library that can be used alongside requests for more efficient web scraping.\n",
        "\n",
        "* As shown below, sometimes specific websites require specific headers in order to process a request to access the data.\n",
        "\n",
        "* To check if a request was processed successfully, use the [status_code](https://requests.readthedocs.io/en/latest/api/) function to see if the process returned 200."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MMxyFjVrKKvG"
      },
      "outputs": [],
      "source": [
        "# API URL and headers in case request gets denied.\n",
        "api_url = \"https://www.ngdc.noaa.gov/hazel/hazard-service/api/v1/volcanoes\"\n",
        "\n",
        "headers = {\n",
        "    'accept': '*/*'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwJMQjBmJ5Ly"
      },
      "source": [
        "#### **TASK 1.0: Webscraping (5 points)**\n",
        "\n",
        "To develop our web scraper, we need to **initiate a GET request** a GET request using the relevant information provided above.\n",
        "\n",
        "This specific NOAA dataset API returns data in JSON format when a request is made. The JSON data is structured in a particular format, so we will extract the necessary information solely from the field named \"items\" to construct a DataFrame.\n",
        "\n",
        "**After successfully scraping the data, name the resulting DataFrame** ***df***\n",
        "\n",
        "**Subsequently, save this DataFrame to a CSV file named volcanoes.csv.**\n",
        "\n",
        "**This process should only need to be executed once.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19vln0EUHr2y",
        "outputId": "d3ee6659-17ec-4df5-8e33-c77312b8b187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500\n"
          ]
        }
      ],
      "source": [
        "response = requests.get(api_url, headers=headers)\n",
        "print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygNz3soJH5HN",
        "outputId": "0041bb08-3b16-4c36-baab-daf88bdaf687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Request failed with status code: 500\n"
          ]
        }
      ],
      "source": [
        "if response.status_code == 200:\n",
        "    data = response.json()\n",
        "    df = pd.DataFrame(data['items'])\n",
        "    df.to_csv('volcanoes.csv', index=False)\n",
        "    print(\"Data saved to volcanoes.csv\")\n",
        "    print(df.shape)\n",
        "else:\n",
        "    print(\"Request failed with status code:\", response.status_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geMtNQeiEo8U"
      },
      "source": [
        "#### **TASK 1.1: 1-Liner Overview (3 points)**\n",
        "To gain an understanding of the dataset's structure, we need to examine some fundamental characteristics of the DataFrame. We need to get an idea of what this dataset is going to look. In order to do that, let's take a look at some of the most [basic things](https://dataanalytics.buffalostate.edu/pandas-cheat-sheet) our dataframe has.\n",
        "\n",
        "\n",
        "***CAN'T USE LOOPS. DO NOT DISPLAY THE DATAFRAME, JUST YOUR CODE OUTPUT HERE.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uyhc9VOJdbn"
      },
      "source": [
        "**1.1.1:** *In one line of code and **using only one function**, show how many **total datapoints and features** there are in the dataframe **together**.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "a_PPlwLyJbbL",
        "outputId": "7773c02e-46be-4ad6-e431-f1dd9a8ae14b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1e339b6a672a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'total datapoints = {df.shape[0]} || total features = {df.shape[1]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "print(f'total datapoints = {df.shape[0]} || total features = {df.shape[1]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoMluxmiESrd"
      },
      "source": [
        "**1.1.2:** *In one line of code, list the **names** of all the **features** in the dataframe.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4XwpjUUETbU"
      },
      "outputs": [],
      "source": [
        "print(list(df.columns))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uvL0VpKNeAI"
      },
      "source": [
        "We won't be using some of the data because there is a lot of missing data.\n",
        "\n",
        "**1.1.3:** *In one line of code, create a **new dataframe** called **new_df** that **contains all** the features of the **old** dataframe **except the following**:*\n",
        "\n",
        "volcanoLocationNum, location, latitude, longitude, agent, significant,\tpublish,\teruption,\tstatus, timeErupt, damageAmountOrder, damageAmountOrderTotal, housesDestroyedAmountOrder,\thousesDestroyedAmountOrderTotal, housesDestroyed,\thousesDestroyedTotal,\tmissingAmountOrder,\tmissingAmountOrderTotal,\tmissing,\tmissingTotal, damageMillionsDollars, damageMillionsDollarsTotal, injuries, injuriesAmountOrder, injuriesTotal, injuriesAmountOrderTotal, deathsAmountOrderTotal, and deathsAmountOrder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmFy8xLFLRrm"
      },
      "outputs": [],
      "source": [
        "new_df = df.drop(columns=[\n",
        "    'volcanoLocationNum', 'location', 'latitude', 'longitude', 'agent',\n",
        "    'significant', 'publish', 'eruption', 'status', 'timeErupt', 'damageAmountOrder',\n",
        "    'damageAmountOrderTotal', 'housesDestroyedAmountOrder', 'housesDestroyedAmountOrderTotal',\n",
        "    'housesDestroyed', 'housesDestroyedTotal', 'missingAmountOrder', 'missingAmountOrderTotal', 'missing',\n",
        "    'missingTotal', 'damageMillionsDollars', 'damageMillionsDollarsTotal', 'injuries', 'injuriesAmountOrder',\n",
        "    'injuriesTotal', 'injuriesAmountOrderTotal', 'deathsAmountOrderTotal',  'deathsAmountOrder'])\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8xP6O37UQp7"
      },
      "source": [
        "#### **TASK 1.2: 1 Liner Shenaniganz (7 points)**\n",
        "\n",
        "We're going to tidy up the **new dataframe** a little more with some more advanced 1 liner code.\n",
        "\n",
        "**Read the directions carefully and code your answer with only one line of code.**\n",
        "\n",
        "**For this section, keep the method of display that is already in the box. Write your code as indicated.**\n",
        "\n",
        "***YOU CAN'T USE ONE LINE LOOPS OR ANY KIND OF LOOP.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_SHOaZRIblA"
      },
      "source": [
        "**1.2.1:** *In one line of code, **discard any row** that contains **NaN** in **any one** of the columns indicating **time**.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B9qVE-OdGysV"
      },
      "outputs": [],
      "source": [
        "new_df.dropna(subset=['year', 'month', 'day'], inplace = True)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO-A8f3gGEpn"
      },
      "source": [
        "**1.2.2:** *In one line of code, **reset** the **index column** of the dataframe so that it has **1-based indexing**.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "gBBUKUJVFI0Y"
      },
      "outputs": [],
      "source": [
        "new_df = new_df.reset_index(drop=True)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSxQRt-xio1t"
      },
      "source": [
        "The **deathsTotal** and **deaths**  columns have approximations of the same data with alternating NaNs in each.\n",
        "\n",
        "**1.2.3:** *In one line of code, make a **new column** called **'totalDeaths'** that takes the **max** of the values given between those* ***two*** *columns. If there is* ***NaN*** *in* ***one column*** *and a* ***numerical*** *value in the* ***other***, *it will ***take the numerical value***. ***Only*** if there are* ***NaNs*** *in* ***both*** *columns, the* ***new column will have NaN.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEZ0YdSPdYyI"
      },
      "outputs": [],
      "source": [
        "new_df['totalDeaths'] = new_df[['deaths', 'deathsTotal']].max(axis=1)\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9if9utm8jsdg"
      },
      "source": [
        "#### **TASK 1.3: Tailoring Time (10 Points)**\n",
        "\n",
        "The year, month, and day columns in the dataset appear to be in an unconventional format. We will need to undertake some fundamental data cleaning on the [time](https://pandas.pydata.org/docs/user_guide/timeseries.html). While more advanced data cleaning techniques will be covered in class, we will focus on basic cleaning for now.\n",
        "\n",
        "**We need to have only ONE column called** \"***date***\" **that contains the full date (YYYY-MM-DD), not separated into three columns.**\n",
        "\n",
        "***Ensure that there are no floating-point values in the date and sort the data from the most recent to the least recent.***\n",
        "\n",
        "***Remove the old columns and place the new column next to the 'id' column.***\n",
        "\n",
        "\n",
        "**YOU MAY USE MULTIPLE LINES OF CODE, BUT CAN'T USE LOOPS.**\n",
        "**Note:** It is alright to have only a **maximum of 12 NaT (Not a Time)s** for some dates that often go further back than the 1600s because the datetime module in Pandas has a limit (unless otherwise guided)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlXlBalOd-Qm"
      },
      "outputs": [],
      "source": [
        "# typecastings\n",
        "new_df['year'] = new_df['year'].astype(int).astype(str)\n",
        "new_df['month'] = new_df['month'].astype(int).astype(str)\n",
        "new_df['day'] = new_df['day'].astype(int).astype(str)\n",
        "\n",
        "# creating new date col\n",
        "new_df['date'] = new_df['year'] + '-' + new_df['month'] + '-' + new_df['day']\n",
        "new_df['date'] = pd.to_datetime(new_df['date'], errors='coerce')\n",
        "\n",
        "# rearranging the cols\n",
        "new_df = new_df[\n",
        "                ['id', 'year', 'month', 'day', 'date', 'tsunamiEventId', 'earthquakeEventId',\n",
        "       'volcanoLocationId', 'volcanoLocationNewNum', 'name', 'country',\n",
        "       'elevation', 'morphology', 'deathsTotal', 'vei', 'deaths',\n",
        "       'totalDeaths']\n",
        "                ]\n",
        "\n",
        "# sorting and dropping\n",
        "new_df = new_df.sort_values(by='date', ascending=False)\n",
        "new_df = new_df.drop(columns=['year', 'month', 'day'])\n",
        "\n",
        "new_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlhYym73iOZb"
      },
      "source": [
        "### **Part 2: Volcanic Matryoshkas  (20 POINTS TOTAL)**\n",
        "\n",
        "Now, that most of the data has been tidied up. We will organize the data into more sizable pieces of information in order to extract useful information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yLjy9u0VuhU"
      },
      "source": [
        "**2.1.1:** *(5 points here)*\n",
        "\n",
        "**Use the groupby function in Pandas to create separate dataframes for each unique country.**\n",
        "\n",
        "* Each table must only have the columns: 'date' 'country', 'name', and 'vei'\n",
        "\n",
        "* Sort the dataframe of each country by highest to lowest 'vei'\n",
        "\n",
        "**You MUST use the groupby function here.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpJznZ-Hfq2C"
      },
      "outputs": [],
      "source": [
        "grouped = new_df.groupby('country')\n",
        "\n",
        "for country, data in grouped:\n",
        "    country_df = data[['date', 'country', 'name', 'vei']].sort_values(by='vei', ascending=False)\n",
        "    print(f\" ############################### DataFrame for {country} ############################### \")\n",
        "    print(country_df)  # Or store these DataFrames in a dictionary or list if needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTExPGKLA7lV"
      },
      "source": [
        "**2.1.2:** *(5 points here)*\n",
        "\n",
        "**Using groupby again, print out the maximum 'vei' for each unique country.**\n",
        "\n",
        "**You MUST use the groupby function here.**\n",
        "\n",
        "* Print out your results in a format like the following: \"Country: {country_name}, Highest VEI: {vei}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ua4ICxVhChM"
      },
      "outputs": [],
      "source": [
        "max_vei_by_country = new_df.groupby('country')['vei'].max()\n",
        "for country, vei in max_vei_by_country.items():\n",
        "    print(f\"Country: {country}, Highest VEI: {vei}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11rdFk2qCu8u"
      },
      "source": [
        "**2.1.3:** *(10 points here)*\n",
        "\n",
        "Finally, we have ALMOST REACHED THE END!!\n",
        "Since there is still quite a bit of missing data, we want to make use of what is still available.\n",
        "\n",
        "A very powerful tool in Python's magnificent collection of libraries is its beautiful graphing tools.\n",
        "\n",
        "Check out libraries such as [Seaborn](https://seaborn.pydata.org/) or [Matplotlib](https://matplotlib.org/stable/index.html) to create meaningful visualizations! **Your final task requires the use of these libraries**\n",
        "\n",
        "**Based on the unique names of volcanos, filter names that have more than 3 datapoints under their name.**\n",
        "\n",
        "* Make separate graphs for each volcano and plot their VEIs over time.\n",
        "\n",
        "* Make sure to properly label all parts of the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvasG8eViN1h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "volcano_counts = new_df['name'].value_counts()\n",
        "volcano_names = volcano_counts[volcano_counts > 3].index\n",
        "\n",
        "for volcano in volcano_names:\n",
        "    volcano_data = new_df[new_df['name'] == volcano]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lineplot(x='date', y='vei', data=volcano_data)\n",
        "\n",
        "    plt.title(f'VEI over Time for {volcano}')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('VEI')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gozs5JwKO7ar"
      },
      "source": [
        "### **Part 3: Fiery Jobs  (15 POINTS TOTAL)**\n",
        "\n",
        "Proficiency in SQL is also super important. SQL databases are essentially relational databases in which there are vast amounts of tabular data. which can often be used to connect with related tablular data. [This](https://www.w3schools.com/sql/) is a pretty good intro into learning more about SQL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfrjZ3wrO-R2"
      },
      "source": [
        "Check out this [tutorial](https://mode.com/sql-tutorial/introduction-to-sql/) for some clarifications on SQL.\n",
        "\n",
        "Now! We'll be using **sqlite** to access a database.\n",
        "* Start by downloading the sql lite file and putting it in the same directory as this [notebook](https://www.kaggle.com/datasets/kaggle/sf-salaries) (hit the 'download' button in the upper right).\n",
        "* Check out the description of the data so you know the table / column names.\n",
        "\n",
        "The following code will use sqlite3 to create a database connection. sqlite3 is the library in Python that assists in navigating through SQL databases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ooUWEVfYPAkD"
      },
      "outputs": [],
      "source": [
        "import sqlite3\n",
        "# import pandas as pd. Pandas was already imported from the previous sections\n",
        "\n",
        "conn = sqlite3.connect(\"database.sqlite\")\n",
        "crsr = conn.cursor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJQ_TWlymAYZ"
      },
      "outputs": [],
      "source": [
        "# uploading the data to the server\n",
        "salary_df = pd.read_csv('Salaries.csv')\n",
        "salary_df.to_sql('Salaries', conn, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnJiNMw7o9d8"
      },
      "outputs": [],
      "source": [
        "# This code will let you check out the different tables within the database.\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = crsr.execute(query).fetchall()\n",
        "print(tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KutbMSr2pV-O"
      },
      "source": [
        "##### **Remember that each problem should be solved with a single sql query.**\n",
        "**All outputs must be shown**\n",
        "\n",
        "#### **3.1.1: 2 Points**\n",
        "***From the Salaries table, get the average base pay for \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) between the year 2011 to 2013.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEoOWNaImLUZ"
      },
      "outputs": [],
      "source": [
        "query = '''SELECT AVG(BasePay)\n",
        "FROM Salaries\n",
        "WHERE JobTitle LIKE '%firefighter%'\n",
        "AND Year BETWEEN 2011 AND 2013\n",
        "'''\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwWzAyYbq9yE"
      },
      "source": [
        "#### **3.1.2: 2 Points**\n",
        "***From the Salaries table, create a table for the year 2014, with a job title of \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) making under $100,000 as a base pay, and sort in descending order by salary.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ_hK1tpq83U"
      },
      "outputs": [],
      "source": [
        "query = '''\n",
        "SELECT *\n",
        "FROM Salaries\n",
        "WHERE JobTitle LIKE '%firefighter%'\n",
        "AND Year = 2014\n",
        "AND BasePay < 100000\n",
        "ORDER BY BasePay DESC\n",
        "'''\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNRnMM7Cq9gG"
      },
      "source": [
        "#### **3.1.3: 4 Points**\n",
        "***Create a dataframe with averages of base pay, averages of benefits, and averages of overtime for \"firefighters\" (all job titles consisting of the word \"firefighter\" (not case sensitive)) as well as a column with the sum of these three values.***\n",
        "\n",
        "***Exclude job titles containing \"FIREFIGHTER\" (case-sensitive)***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVOCgk2Jq9LB"
      },
      "outputs": [],
      "source": [
        "query = '''\n",
        "SELECT\n",
        "   AVG(BasePay) AS AverageBasePay,\n",
        "   AVG(Benefits) AS AverageBenefits,\n",
        "   AVG(OvertimePay) AS AverageOvertimePay,\n",
        "   AVG(BasePay) + AVG(Benefits) + AVG(OvertimePay) AS TotalAverageCompensation\n",
        "FROM Salaries\n",
        "WHERE JobTitle LIKE '%firefighter%'\n",
        "AND JobTitle != 'FIREFIGHTER'\n",
        "'''\n",
        "\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cp88tfxq-Ui"
      },
      "source": [
        "#### **3.1.4: 7 Points**\n",
        "\n",
        "***Finally, we'll create our own table in our database. Separate the Salaries table by years, and add it back to the database. Using a loop might be helpful.***\n",
        "\n",
        "* You may use basic python to complete the task. However, using querying on SQL is **mandatory**.\n",
        "* Feel free to **use multiple lines of code for this problem only.**\n",
        "* Check out this [Hint](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjQnj4Uqq9C7"
      },
      "outputs": [],
      "source": [
        "for year in salary_df['Year'].unique():\n",
        "    year_df = salary_df[salary_df['Year'] == year]\n",
        "    year_df.to_sql(f'Salaries_{year}', conn, if_exists='replace', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYcadk9g4jE1"
      },
      "outputs": [],
      "source": [
        "# Run this code to check if you successfully added your table.\n",
        "cursor = conn.cursor()\n",
        "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "print(cursor.fetchall())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW-M2Epk28g4"
      },
      "source": [
        "![volcano](https://as1.ftcdn.net/v2/jpg/06/34/76/64/1000_F_634766457_0fZbpYj6aBLlldO1jADUPpKTRLnNmngs.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Kmist_PFIZ"
      },
      "source": [
        "### **Part 4: Be Creative with SQL  (20 POINTS TOTAL)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC7vPgQdPO9R"
      },
      "source": [
        "For a more interactive SQL experience within a Jupyter notebook or Google Colab, you can use the %sql magic command from the ipython-sql extension. Here's how:\n",
        "\n",
        "* Install ipython-sql: !pip install ipython-sql\n",
        "* Load the extension and connect to SQLite:\n",
        "\n",
        "%load_ext sql\n",
        "\n",
        "%sql sqlite:///restaurent.db\n",
        "\n",
        "\n",
        "* Run SQL queries directly in cells using\n",
        "\n",
        "%%sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5CXyWVOP4x5"
      },
      "source": [
        "However, you can also continue to import the sqlite3 library and create a connection and cursor to execute SQL commands, as you did before. The choice is up to you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txAdbUgoQvAI"
      },
      "source": [
        "*** Part 4.1(a)  CREATE A TABLE ***\n",
        "\n",
        "Create a table called **MyRestaurants** with the following attributes:\n",
        "\n",
        "* RestaurantName: A VARCHAR field\n",
        "* FoodType: A VARCHAR field\n",
        "* DistanceFromHome: An INTEGER field representing the distance (in minutes) from your house\n",
        "* LastVisitDate: A DATE field representing the date of your last visit\n",
        "* Liked: A BOOLEAN field indicating whether you like the restaurant or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJVjTopqQnXI"
      },
      "outputs": [],
      "source": [
        "sql = '''CREATE TABLE MyRestaurants (\n",
        "  RestaurantName VARCHAR(255),\n",
        "  FoodType VARCHAR(255),\n",
        "  DistanceFromHome INTEGER,\n",
        "  LastVisitDate DATE,\n",
        "  Liked BOOLEAN\n",
        ");\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGXjBWD7sIkD"
      },
      "outputs": [],
      "source": [
        "# This code will let you check out the different tables within the database.\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = crsr.execute(query).fetchall()\n",
        "print(tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLcTxvs-Q3GT"
      },
      "source": [
        "*** Part 4.1(b) Insert Tuples ***\n",
        "\n",
        "Insert at least five tuples using the SQL INSERT command, executing it five (or more) times.\n",
        "\n",
        "* Ensure that you insert ** at least one restaurant that you liked**, **at least one restaurant that you did not like**, and **at least one restaurant where the Liked field is set to NULL**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K56Wr-oKsb9N"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO MyRestaurants\n",
        "(RestaurantName, FoodType, DistanceFromHome, LastVisitDate, Liked)\n",
        "VALUES ('Mc Burgers', 'American', 45, '2024-05-01', 0);'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQu_K84fudT8"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO MyRestaurants\n",
        "(RestaurantName, FoodType, DistanceFromHome, LastVisitDate, Liked)\n",
        "VALUES ('House of Sushis', 'Japanese', 105, '2024-06-10', 1);'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uh2nVlTRsbwb"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO MyRestaurants\n",
        "(RestaurantName, FoodType, DistanceFromHome, LastVisitDate, Liked)\n",
        "VALUES ('Taco Bell', 'Mexican', 920, '2024-07-15', 0);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1jhHrfBsbpL"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO MyRestaurants\n",
        "(RestaurantName, FoodType, DistanceFromHome, LastVisitDate, Liked)\n",
        "VALUES ('Momo King', 'Thai', 425, '2024-08-01', 1);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sjy1mbAsb2r"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO MyRestaurants\n",
        "(RestaurantName, FoodType, DistanceFromHome, LastVisitDate, Liked)\n",
        "VALUES ('Pizza Hut', 'Italian', 110, '2024-09-12', 1);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AlFQ7NSRA1x"
      },
      "source": [
        "*** Part 4.1(c) ***\n",
        "\n",
        "Write a SQL query to return all restaurants in your table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzXPdd0QRG81"
      },
      "outputs": [],
      "source": [
        "query = '''SELECT *\n",
        "FROM MyRestaurants\n",
        "'''\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wglVgmgRHap"
      },
      "source": [
        "*** Part 4.1(d) ***\n",
        "\n",
        "Now experiment with a few of SQLite's output formats using the SQL query you wrote for question above:\n",
        "\n",
        "* print the results (Part 4.1(c)) in list form, delimited by \" | \"\n",
        "\n",
        "Sample output: Sushi Place | Japanese | 15 | 2024-05-01 | 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7RqnXA6RVWe"
      },
      "outputs": [],
      "source": [
        "sql = '''SELECT\n",
        "  RestaurantName || ' | ' ||\n",
        "  FoodType || ' | ' ||\n",
        "  DistanceFromHome || ' | ' ||\n",
        "  LastVisitDate || ' | ' ||\n",
        "  Liked as temp\n",
        "FROM MyRestaurants;\n",
        "'''\n",
        "df = pd.read_sql(sql, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy8KzdRNRb2B"
      },
      "source": [
        "*** Part 4.1(d) ***\n",
        "\n",
        "Modify your SQL query such that it prints \"I liked it\" or \"I hated it\" for each restaurant you liked or not.\n",
        "\n",
        "\n",
        "Note that you are not allowed to modify the table on disk. You should be able to answer this question using only a SELECT statement. A solution that creates and uses an extra table, howerver, will be accepted.\n",
        "\n",
        "Check: https://www.sqlitetutorial.net/sqlite-case/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MX4VPCXuR3dw"
      },
      "outputs": [],
      "source": [
        "sql = '''SELECT\n",
        "  RestaurantName,\n",
        "  CASE\n",
        "    WHEN Liked = 1 THEN 'I liked it'\n",
        "    WHEN Liked = 0 THEN 'I hated it'\n",
        "    ELSE 'I have no opinion'\n",
        "  END AS MyOpinion\n",
        "FROM MyRestaurants;\n",
        "'''\n",
        "df = pd.read_sql(sql, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91z74HBMR5Lo"
      },
      "source": [
        "*** Part 4.1(e) ***\n",
        "\n",
        "Write a SQL query to return all restaurants that you like but have not visited in the past three months."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-1rbnubR-9l"
      },
      "outputs": [],
      "source": [
        "sql = '''SELECT RestaurantName\n",
        "FROM MyRestaurants\n",
        "WHERE Liked = 1\n",
        "AND LastVisitDate < date('now', '-3 months');\n",
        "'''\n",
        "df = pd.read_sql(sql, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GObL-sWeSCBl"
      },
      "source": [
        "*** Part 4.2(a) ***\n",
        "\n",
        "** Next we will focus on INNER JOIN **\n",
        "\n",
        "Let's add a second table called **\"RestaurantReviews\"** and demonstrate how to perform an inner join between MyRestaurants and RestaurantReviews.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmjWpt48SLsc"
      },
      "source": [
        "Create the RestaurantReviews Table\n",
        "First, we'll create the RestaurantReviews table with the following attributes:\n",
        "\n",
        "* RestaurantName (to match the Name in MyRestaurants)\n",
        "* ReviewText (text of the review)\n",
        "* Rating (integer rating)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN24uq2HSMN8"
      },
      "outputs": [],
      "source": [
        "sql = '''CREATE TABLE RestaurantReviews (\n",
        "  RestaurantName VARCHAR(255),\n",
        "  ReviewText TEXT,\n",
        "  Rating INTEGER\n",
        ");\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2DJI_WwOvs7M"
      },
      "outputs": [],
      "source": [
        "# This code will let you check out the different tables within the database.\n",
        "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
        "tables = crsr.execute(query).fetchall()\n",
        "print(tables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y99cEJBHSMp9"
      },
      "source": [
        "*** Part 4.2(b) ***\n",
        "\n",
        "Insert Data into RestaurantReviews\n",
        "Insert some sample data (at least 5) into this table:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlyp_w4SSx-"
      },
      "outputs": [],
      "source": [
        "sql = '''\n",
        "INSERT INTO RestaurantReviews\n",
        "(RestaurantName, ReviewText, Rating)\n",
        "VALUES ('House of Sushis', 'The best sushi in town! Fresh and delicious.', 5);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7E4HtN-sv5hk"
      },
      "outputs": [],
      "source": [
        "sql = '''INSERT INTO RestaurantReviews\n",
        "(RestaurantName, ReviewText, Rating)\n",
        "VALUES ('Mc Burgers', 'Decent burgers, but the fries were cold.', 3);'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lGDXzFIv5bW"
      },
      "outputs": [],
      "source": [
        "sql = '''\n",
        "INSERT INTO RestaurantReviews\n",
        "(RestaurantName, ReviewText, Rating)\n",
        "VALUES ('Taco Bell', 'Classic Taco Bell - always hits the spot.', 4);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlM13Qwmv5TO"
      },
      "outputs": [],
      "source": [
        "sql = '''\n",
        "INSERT INTO RestaurantReviews\n",
        "(RestaurantName, ReviewText, Rating)\n",
        "VALUES ('Momo King', 'Amazing momos and friendly service.', 5);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mOJhfiWNv5Mg"
      },
      "outputs": [],
      "source": [
        "sql = '''\n",
        "INSERT INTO RestaurantReviews\n",
        "(RestaurantName, ReviewText, Rating)\n",
        "VALUES ('Pizza Hut', 'Good pizza, but a bit pricey.', 4);\n",
        "'''\n",
        "crsr.execute(sql)\n",
        "conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_L5js02wpP1"
      },
      "outputs": [],
      "source": [
        "query = '''SELECT *\n",
        "FROM RestaurantReviews\n",
        "'''\n",
        "df = pd.read_sql(query, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5RGDTfrSU6F"
      },
      "source": [
        "*** Part 4.2(c) Inner Join Query ***\n",
        "Now perform an inner join between MyRestaurants and RestaurantReviews based on the restaurant name to combine the information from both tables and show the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCBzaVCkSZ7F"
      },
      "outputs": [],
      "source": [
        "sql = '''\n",
        "SELECT a.*, b.*\n",
        "FROM RestaurantReviews as a\n",
        "\n",
        "INNER JOIN MyRestaurants as b\n",
        "ON a.RestaurantName = b.RestaurantName;\n",
        "'''\n",
        "df = pd.read_sql(sql, conn)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84e5h-3WScF9"
      },
      "source": [
        "## THE END OF HW 1 ##"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HrD3qZmJUp6C",
        "IlhYym73iOZb",
        "Gozs5JwKO7ar",
        "KutbMSr2pV-O",
        "AwWzAyYbq9yE",
        "zNRnMM7Cq9gG",
        "0cp88tfxq-Ui"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
